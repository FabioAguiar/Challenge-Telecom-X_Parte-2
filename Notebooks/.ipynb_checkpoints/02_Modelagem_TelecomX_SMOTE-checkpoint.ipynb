{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae347a29",
   "metadata": {},
   "source": [
    "# Modelagem Preditiva de Churn – Telecom X (Parte 2)\n",
    "\n",
    "> **Objetivo:** treinar, avaliar e comparar modelos de classificação para prever **evasão** (*churn*), \n",
    "mantendo o **pipeline reprodutível** e evitando *data leakage*.\n",
    "Este notebook **preserva a narrativa explicativa** (títulos + bullets) e inclui **oversampling com SMOTE** dentro do pipeline (opcional).\n",
    "\n",
    "**Sumário**\n",
    "1. Contexto e premissas\n",
    "2. Carregamento do dataset base\n",
    "3. Target, *features* e distribuição de classes\n",
    "4. Split estratificado (treino/teste)\n",
    "5. Pré-processamento (One-Hot + StandardScaler)\n",
    "6. Pipelines **com** e **sem** SMOTE\n",
    "7. Validação cruzada (5-fold) e quadro comparativo\n",
    "8. Ajuste final e avaliação em teste (métricas, matriz de confusão, ROC)\n",
    "9. Importâncias (Random Forest)\n",
    "10. Conclusões rápidas e próximos passos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62201a1",
   "metadata": {},
   "source": [
    "## 1) Contexto e premissas\n",
    "- O arquivo `dataset_base.csv` foi exportado no **Notebook 01**.\n",
    "- A coluna alvo se chama **`Evasao`** (0 = não evadiu, 1 = evadiu).\n",
    "- O **SMOTE** será aplicado **apenas no conjunto de treino** (e dentro do pipeline) para evitar *data leakage*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b69b8c0",
   "metadata": {},
   "source": [
    "### Pacotes e configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, RocCurveDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "pd.set_option(\"display.max_columns\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a395c9",
   "metadata": {},
   "source": [
    "## 2) Carregamento do dataset base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"dataset_base.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d5ab18",
   "metadata": {},
   "source": [
    "## 3) Target, *features* e distribuição de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca58338",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"Evasao\"  # ajuste aqui se preferir renomear para 'Churn' no Notebook 01\n",
    "\n",
    "assert TARGET_COL in df.columns, f\"Coluna alvo '{TARGET_COL}' não encontrada.\"\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "print(\"Distribuição do target (absoluta):\")\n",
    "print(y.value_counts())\n",
    "\n",
    "print(\"\\nDistribuição do target (proporção):\")\n",
    "print((y.value_counts(normalize=True)*100).round(2).astype(str) + \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e30b7",
   "metadata": {},
   "source": [
    "## 4) Split estratificado (treino/teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a67aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print(\"Shapes -> X_train:\", X_train.shape, \"| X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3aea1",
   "metadata": {},
   "source": [
    "## 5) Pré-processamento (One-Hot + StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Categóricas ({len(cat_cols)}):\", cat_cols[:20], \"...\" if len(cat_cols)>20 else \"\")\n",
    "print(f\"Numéricas   ({len(num_cols)}):\", num_cols[:20], \"...\" if len(num_cols)>20 else \"\")\n",
    "\n",
    "preprocess_lr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse=False), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess_tree = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(with_mean=False), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False), cat_cols),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf29708",
   "metadata": {},
   "source": [
    "## 6) Pipelines **com** e **sem** SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97279dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LogReg\": LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=11),\n",
    "}\n",
    "\n",
    "pipelines_no_smote = {\n",
    "    \"LogReg_noSMOTE\": Pipeline(steps=[(\"preprocess\", preprocess_lr), (\"model\", models[\"LogReg\"])]),\n",
    "    \"RF_noSMOTE\":     Pipeline(steps=[(\"preprocess\", preprocess_tree), (\"model\", models[\"RandomForest\"])]),\n",
    "    \"KNN_noSMOTE\":    Pipeline(steps=[(\"preprocess\", preprocess_lr), (\"model\", models[\"KNN\"])]),\n",
    "}\n",
    "\n",
    "pipelines_smote = {\n",
    "    \"LogReg_SMOTE\": ImbPipeline(steps=[(\"preprocess\", preprocess_lr), (\"smote\", SMOTE(random_state=RANDOM_STATE)), (\"model\", models[\"LogReg\"])]),\n",
    "    \"RF_SMOTE\":     ImbPipeline(steps=[(\"preprocess\", preprocess_tree), (\"smote\", SMOTE(random_state=RANDOM_STATE)), (\"model\", models[\"RandomForest\"])]),\n",
    "    \"KNN_SMOTE\":    ImbPipeline(steps=[(\"preprocess\", preprocess_lr), (\"smote\", SMOTE(random_state=RANDOM_STATE)), (\"model\", models[\"KNN\"])]),\n",
    "}\n",
    "\n",
    "list(pipelines_no_smote.keys()), list(pipelines_smote.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf61cec",
   "metadata": {},
   "source": [
    "## 7) Validação cruzada (5-fold) e quadro comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\"accuracy\":\"accuracy\",\"precision\":\"precision\",\"recall\":\"recall\",\"f1\":\"f1\",\"roc_auc\":\"roc_auc\"}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "def run_cv(pipelines_dict, label):\n",
    "    cv_results = {}\n",
    "    for name, pipe in pipelines_dict.items():\n",
    "        scores = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
    "        cv_results[name] = {m: (scores[f\"test_{m}\"].mean(), scores[f\"test_{m}\"].std()) for m in scoring}\n",
    "    df_cv = pd.DataFrame({m: {k: f\"{v[m][0]:.3f} ± {v[m][1]:.3f}\" for k, v in cv_results.items()} for m in scoring})\n",
    "    df_cv = df_cv.T\n",
    "    df_cv.index.name = f\"Métrica ({label})\"\n",
    "    return df_cv\n",
    "\n",
    "cv_no = run_cv(pipelines_no_smote, \"sem SMOTE\")\n",
    "cv_sm = run_cv(pipelines_smote, \"com SMOTE\")\n",
    "\n",
    "print(\"=== Validação Cruzada (SEM SMOTE) ===\")\n",
    "display(cv_no)\n",
    "print(\"\\n=== Validação Cruzada (COM SMOTE) ===\")\n",
    "display(cv_sm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4300478",
   "metadata": {},
   "source": [
    "## 8) Ajuste final e avaliação em teste (métricas, matriz de confusão, ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot(pipe, X_train, y_train, X_test, y_test, title):\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_proba = pipe.predict_proba(X_test)[:, 1] if hasattr(pipe, \"predict_proba\") else None\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "\n",
    "    print(f\"{title} -> Accuracy: {acc:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f} | ROC-AUC: {roc:.3f}\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Matriz de Confusão:\\n\", cm)\n",
    "\n",
    "    if y_proba is not None:\n",
    "        RocCurveDisplay.from_predictions(y_test, y_proba)\n",
    "        plt.title(f\"Curva ROC - {title}\")\n",
    "        plt.show()\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"roc_auc\": roc}\n",
    "\n",
    "models_to_test = [\n",
    "    (\"RF_noSMOTE\", pipelines_no_smote[\"RF_noSMOTE\"]),\n",
    "    (\"RF_SMOTE\", pipelines_smote[\"RF_SMOTE\"]),\n",
    "    (\"LogReg_noSMOTE\", pipelines_no_smote[\"LogReg_noSMOTE\"]),\n",
    "    (\"LogReg_SMOTE\", pipelines_smote[\"LogReg_SMOTE\"]),\n",
    "]\n",
    "\n",
    "final_results = {}\n",
    "for name, pipe in models_to_test:\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    final_results[name] = evaluate_and_plot(pipe, X_train, y_train, X_test, y_test, title=name)\n",
    "\n",
    "results_df = pd.DataFrame(final_results).T.sort_values(\"f1\", ascending=False)\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be01944b",
   "metadata": {},
   "source": [
    "## 9) Importâncias (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0bbbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_smote = pipelines_smote[\"RF_SMOTE\"]\n",
    "rf_smote.fit(X_train, y_train)\n",
    "\n",
    "ohe = rf_smote.named_steps[\"preprocess\"].named_transformers_[\"cat\"]\n",
    "cat_feature_names = list(ohe.get_feature_names_out()) if hasattr(ohe, \"get_feature_names_out\") else [f\"cat_{i}\" for i in range(len(X.columns))]\n",
    "feature_names = num_cols + cat_feature_names\n",
    "\n",
    "rf = rf_smote.named_steps[\"model\"]\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "imp_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances}).sort_values(\"importance\", ascending=False).head(20)\n",
    "display(imp_df)\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(imp_df[\"feature\"][::-1], imp_df[\"importance\"][::-1])\n",
    "plt.title(\"Top 20 Importâncias - Random Forest (com SMOTE)\")\n",
    "plt.xlabel(\"Importância\")\n",
    "plt.ylabel(\"Variável\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ba029",
   "metadata": {},
   "source": [
    "\n",
    "### 10-A. Análise de Importância das Variáveis (múltiplos métodos)\n",
    "\n",
    "Nesta seção, complementamos a interpretação dos modelos:\n",
    "- **Random Forest**: importância por redução de impureza (já implementada).\n",
    "- **Regressão Logística**: coeficientes (após pré-processamento).\n",
    "- **KNN**: *Permutation Importance* (importância por embaralhamento), já que o KNN não possui atributos nativos de importância.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_feature_names_from_preprocess(preprocess, num_cols, cat_cols):\n",
    "    # Recupera nomes após transformação\n",
    "    cat_transformer = preprocess.named_transformers_.get(\"cat\", None)\n",
    "    if cat_transformer is not None and hasattr(cat_transformer, \"get_feature_names_out\"):\n",
    "        cat_names = list(cat_transformer.get_feature_names_out(cat_cols))\n",
    "    else:\n",
    "        cat_names = [f\"{c}_encoded\" for c in cat_cols]\n",
    "    return num_cols + cat_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importância via coeficientes (Regressão Logística) no pipeline SEM SMOTE para evitar viés do oversampling\n",
    "logreg_pipe = pipelines_no_smote.get(\"LogReg_noSMOTE\")\n",
    "if logreg_pipe is not None:\n",
    "    # Ajustar no treino\n",
    "    logreg_pipe.fit(X_train, y_train)\n",
    "    feature_names = get_feature_names_from_preprocess(\n",
    "        logreg_pipe.named_steps[\"preprocess\"], num_cols, cat_cols\n",
    "    )\n",
    "    clf = logreg_pipe.named_steps[\"model\"]\n",
    "    coefs = clf.coef_.ravel()\n",
    "    imp_lr = pd.DataFrame({\"feature\": feature_names, \"coef\": coefs, \"abs_coef\": np.abs(coefs)})                 .sort_values(\"abs_coef\", ascending=False).head(20)\n",
    "    display(imp_lr)\n",
    "else:\n",
    "    print(\"Pipeline LogReg_noSMOTE não encontrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importância por permutação para KNN (pipeline SEM SMOTE, medindo queda média no F1 em validação interna)\n",
    "knn_pipe = pipelines_no_smote.get(\"KNN_noSMOTE\")\n",
    "if knn_pipe is not None:\n",
    "    # Ajustar no treino\n",
    "    knn_pipe.fit(X_train, y_train)\n",
    "    # Permutation importance no conjunto de TESTE transformado pelo pipeline completo\n",
    "    result = permutation_importance(\n",
    "        knn_pipe, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, scoring=\"f1\", n_jobs=-1\n",
    "    )\n",
    "    feature_names = get_feature_names_from_preprocess(\n",
    "        knn_pipe.named_steps[\"preprocess\"], num_cols, cat_cols\n",
    "    )\n",
    "    imp_knn = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance_mean\": result.importances_mean,\n",
    "        \"importance_std\": result.importances_std\n",
    "    }).sort_values(\"importance_mean\", ascending=False).head(20)\n",
    "    display(imp_knn)\n",
    "else:\n",
    "    print(\"Pipeline KNN_noSMOTE não encontrado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49c32d",
   "metadata": {},
   "source": [
    "\n",
    "> **Nota:** Importâncias podem variar entre técnicas.  \n",
    "> Use **coeficientes** para modelos lineares, **redução de impureza** para árvores e **permutation importance** como abordagem agnóstica ao modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92bfaf",
   "metadata": {},
   "source": [
    "## 10) Conclusões rápidas e próximos passos\n",
    "- Compare as tabelas de cross-validation **com** e **sem** SMOTE. Observe **Recall** e **F1**.\n",
    "- Se o objetivo de negócio for **reduzir falsos negativos** (não deixar evadir), priorize **Recall** (e considere ajustar limiar de decisão).\n",
    "- Próximos passos:\n",
    "  - *GridSearchCV* para refinar hiperparâmetros do(s) melhor(es) pipeline(s).\n",
    "  - SHAP para interpretabilidade local.\n",
    "  - Exportar o pipeline final com `joblib`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
